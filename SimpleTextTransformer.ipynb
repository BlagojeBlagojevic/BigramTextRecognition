{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib"
      ],
      "metadata": {
        "id": "kdGTUUTU-lNB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFKP3Lv2U7if",
        "outputId": "47f9e2f3-b037-4f1c-9b1b-3b778355ceba"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-04 11:55:40--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-01-04 11:55:41 (24.5 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HYPER PARMETARS\n",
        "batch_size = 16 #Kad se kaze batch misli se na koliko sekvenci paralelno cemo gledati sum gradient\n",
        "block_size = 32 #Koliko karaktera prije cemo gledati\n",
        "max_iter = 1000 #Koliko training loop\n",
        "eval_interval = 100 #Kad se vrsi provjera gubitaka\n",
        "learning_rate = 1e-3\n",
        "eval_iters = 200\n",
        "n_embeds = 64\n",
        "n_layer = 8\n",
        "n_heads = 4\n",
        "dropout = 0\n",
        "\n",
        "#\n",
        "\n",
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  text =f.read()\n",
        "\n",
        "vocab_size  = 255\n"
      ],
      "metadata": {
        "id": "vUwiVrZ-VJQz"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder(text):\n",
        "  encoded = []\n",
        "  for i in range(len(text)):\n",
        "    encoded.append(ord(text[i]))\n",
        "  return encoded\n",
        "#encoded = encoder(text)\n",
        "\n",
        "def decoder(encoded):\n",
        "  decoded = []\n",
        "  for i in range(len(encoded)):\n",
        "    decoded.append(chr(encoded[i]))\n",
        "  return decoded\n",
        "encoded = encoder(text)\n",
        "data = torch.tensor(encoded)  # Data pretstavlja tensor koji cemo koristiti\n",
        "n = int(len(data)*0.9) #90% training 10% validacija\n",
        "trainig_data=data[:n]\n",
        "val_data=data[n:]\n",
        "#Zavisi koji je split training ili validacijoni\n",
        "def get_batch(split):\n",
        "  data = trainig_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  #x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "#x,y = get_batch(\"train\")\n"
      ],
      "metadata": {
        "id": "ztOu9SdFZz0s"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Head(torch.nn.Module):\n",
        "  # ---------------------------------------- #\n",
        "  def __init__(self, head_size) -> None:   #Prima n_heads u konstruktoru register_baffer je nasledjeno iz Module\n",
        "     super().__init__()\n",
        "     self.key = torch.nn.Linear(n_embeds,head_size,bias = False)\n",
        "     self.query  = torch.nn.Linear(n_embeds,head_size,bias = False)\n",
        "     self.value = torch.nn.Linear(n_embeds,head_size,bias = False)\n",
        "     self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "     self.dropout = torch.nn.Dropout(dropout)\n",
        "  # ---------------------------------------- #\n",
        "  def forward(self,x):\n",
        "    B,T,C = x.shape   #Vadi batch, time, chanels\n",
        "    k = self.key(x)   #DIM B,T,C\n",
        "    q = self.query(x) #DIM B,T,C\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5 #C**-0.5 Je korijen iz broja dimenzija\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # Da sakrijemo elemente poslije usustini matrica\n",
        "                                                                  # Gdje su svi brojevi ispod glavne avrage of sum of\n",
        "    wei = torch.nn.functional.softmax(wei, dim=1)\n",
        "    wei = self.dropout(wei)     #Valjda pomaze pri treningu Random postavi neki ulaz na nulu\n",
        "    v = self.value(x) #\n",
        "    out = wei @ v\n",
        "    return out\n",
        "   # ---------------------------------------- #\n",
        "\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):           #Postavlja vise glava za procesovanje\n",
        "        super().__init__()\n",
        "        self.heads = torch.nn.ModuleList([Head(head_size) for _ in range(num_heads)]) #Module liste svih modula unutar klasa\n",
        "        self.proj = torch.nn.Linear(n_embeds, n_embeds)  #Jos jedan layer zbog learninga\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(torch.nn.Module):  #Fead forward model poslije\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_embd, 4 * n_embd),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(4 * n_embd, n_embd),\n",
        "            torch.nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x): #Ide kroz\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(torch.nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)   # Multi head atention\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = torch.nn.LayerNorm(n_embd)   # Normalizacija meen 0 var 1\n",
        "        self.ln2 = torch.nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))    ## + Pomaze rasporediti gradient unutar nn\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class SimpleLanguageModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = torch.nn.Embedding(vocab_size, n_embeds)\n",
        "        self.position_embedding_table =  torch.nn.Embedding(block_size, n_embeds)\n",
        "        self.blocks = torch.nn.Sequential(*[Block(n_embeds, n_head=n_heads) for _ in range(n_layer)])\n",
        "        self.ln_f =  torch.nn.LayerNorm(n_embeds) # final layer norm\n",
        "        self.lm_head =  torch.nn.Linear(n_embeds, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T)) #\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss =  torch.nn.functional.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs =  torch.nn.functional.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = SimpleLanguageModel()\n",
        "#m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iter):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iter - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, )\n",
        "a = decoder(model.generate(context, max_new_tokens=2000)[0].tolist())\n",
        "s = ''.join(a)\n",
        "print(s)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE269dFLfgcb",
        "outputId": "312447fe-8b65-4414-af71-55ef2051d74e"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.433407 M parameters\n",
            "step 0: train loss 5.6338, val loss 5.6357\n",
            "step 100: train loss 2.6728, val loss 2.6740\n",
            "step 200: train loss 1.9661, val loss 1.9788\n",
            "step 300: train loss 1.0608, val loss 1.0887\n",
            "step 400: train loss 0.5636, val loss 0.5941\n",
            "step 500: train loss 0.3049, val loss 0.3278\n",
            "step 600: train loss 0.2207, val loss 0.2334\n",
            "step 700: train loss 0.1728, val loss 0.1850\n",
            "step 800: train loss 0.1305, val loss 0.1420\n",
            "step 900: train loss 0.1161, val loss 0.1211\n",
            "step 999: train loss 0.1128, val loss 0.1169\n",
            "\u0000Údlllllllllllllllllyllyylllssed:uy, alveoelres,ueflvel, ryno, yey!o,\n",
            "Pa ise rou,d,\n",
            "S they yh.A:\n",
            "Thetorredle, llre seou yo houre, veuyully hy hof,erl, proore healreo s,ulely derey yordere dorus yly e hillerp,hLNo our wyiecoudesifl, efroed seafy, urdureye dtheuredue,' chah,ull aket, mo cinouk y mheneorush;o, foh uriul yleos tulure herpewllevelerkereereGee:alss, eor ind,\n",
            "Tisrt Etdehe mey regly,perale,Thlid,\n",
            "MFrulereroe deuremer keeveuy oo ilceulld, actheeuv,\n",
            "AGo noardvere, omhand-ousesl, sie mallro ket,uyrese:uucornd wyor,e yreere\u0006 aery ley ulfored len weuorey repy noorr,r, Ho hyuy horeryriley c, co oueduuyeosureuro, surure hionid ruoudsel, amodreus,\n",
            "\n",
            "f Couevy hers\n",
            "Bulelvt meeleyrod, yelet,\n",
            "ChLoa ur lyre de coreuy, led he oiai seu nist Ie Wllrery: kyre rome, qh, as wyry yo llay dey,u anor, ause lyh leaevel, ,luvys:\n",
            "Mry, yosu omee \n",
            "rour weproudeh: hetheurey hef holo or, ehovue,\n",
            "\n",
            "Gerole aiw,ude' hno lou,d, hieverurhes;dery lemro,py, cwumere; heel ripare topra aneever lly souneloe?d ptothe yrery, yoeld wrerere il ydurllduy sot hyre herleeuridrefereery ayd roou elld oyor?pe,xsuhey,Touryr. erouev,wuleryre oowe ule ofre yeall.\n",
            "I cere, a;prot bory bals,\n",
            "La,d kond urerere, touled plerdre,ch houredeered, he sromeers, sorrelels, eeu,w \n",
            "isuo ree qre hlyoon ,reeefhen,l ayer,c: aureerply, operllle'd, ueyd tyT, pea ous Crefarkist:htheudrodurell,fry, se,urd,  ifheitge,py a,rAre,, ,\n",
            "Iheo inyrury ouryou Iore, u bliveel hel.\n",
            "TSyerey hele ke.\n",
            "\n",
            "wo He, ill ylellorde,-, peory Tserooueh, arshens,Dle fe he.\n",
            "H,rd peave orecherpllealy pore, lo\n",
            "Toufeur,' ro,us wevy,\n",
            "Hlfre locom treeld anlu yelel, ylyorl uoyrure yre Weend sdvereole sallk weu,t rpeeed coulrery eceesss hu o lerdrere or,f stece py tolou,dre's, afrevy heel, heuyylo ware , there creey oor, wd inof pil, ly yereve sill, eayaer', pere o her,\n",
            "IAveuthe yleaylelouly ton, mehpy eor, rered olore e hed, dhrely,\n",
            "Toumeer;reves,\n",
            "Breeve h, mryrerd s pono lomy oedrerd, oly, uIos dallellneverlurerelduully, aree det bliryoromy as nooune werear, heso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# generate from the model\n",
        "#$context = torch.zeros((1, 1), dtype=torch.long, )\n",
        "#print(decoder(model.generate(context, max_new_tokens=1000)[0].tolist()).join())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQnEhxgtuYli",
        "outputId": "2c6b864a-edbf-4055-9a7f-d988928b7a4c"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u0000\n",
            "Haaasllllbuuuuupueellllll;  isal llolliessot!\n",
            "TYsho lio 'lgound o hirin gath datotat pcirizetshe t athe co atigace ha tathe onas\n",
            "y he mus y hall;\n",
            "Hast fifh thathre sol\n",
            " tou Iosn\n",
            "ur tue, thie har do hst hath th hat bawh Dis h bimave fhad hayily it thray se es uithig b ato at taro prthatavathe hin wkigls awiury igt.\n",
            "I ot oigut tha, te bou't; i hamit lanas\n",
            "orict heis thiÃwill tÜy ut-thtti irathe tathe y Jtouur pe bldt haw wacthe ate to-tht hares un thary b toous\n",
            "Bwhon'fer wamtigh to is ma moslse; tat isk si it ha thit bet\n",
            "E(nh an kebr's ha boub, tibld!\n",
            "Phes may?\n",
            "C wetlphe is\n",
            "Sishe atithit Sitt,\n",
            "thive: tit meaisak sch st ath haseries th koofr to d nuwe liliss weaW say k wawi the masid\n",
            "At thimet a sopeeangh theals apiest you ad, tha rsio thouÅuk isk ous im youdh ary ou arindrs h.\n",
            "\n",
            "PEUK\n",
            "TSho lethe y hi micapah reies at ot imeahatr ue oure atalle urs thist-\n",
            "oinnd\n",
            "ed the heant\n",
            "2 wove y pir hith the y ast'e,\n",
            "As\n",
            "h haplee urs pod wis bl-hesn; ink toet ou l; thealausse h the:\n",
            "Sbr; hat id \n",
            "he h\n"
          ]
        }
      ]
    }
  ]
}